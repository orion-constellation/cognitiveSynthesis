{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threat Hunting for Gretel Processing\n",
    "Hugging Face Dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jupyter ipykernel\n",
    "import os\n",
    "#VIRTUALENVGOODNESS - YEAH BRAHHHHHH\n",
    "ROOT_DIR = '../'\n",
    "\n",
    "VIRTUALENV=True\n",
    "#################\n",
    "VENV_NAME=\"venv\"\n",
    "if VIRTUALENV == True:\n",
    "    try:\n",
    "        os.chdir(ROOT_DIR)\n",
    "        print(os.path.curdir)\n",
    "        !source $VENV_NAME/bin/activate\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        !pip3 install virtualenv\n",
    "        !virtualenv $VENV_NAME\n",
    "        !source $VENV_NAME/bin/activate\n",
    "        %python3 -m ipykernel install  --user --name=\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install -q wandb #comet_ml\n",
    "%pip install pyarrow python-dotenv datasets gretel-client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS=0\n",
    "PROJECT_NAME=\"Cognitive Synthesis\"\n",
    "DEBUG = True\n",
    "LOG_LEVEL = DEBUG\n",
    "PARALLEL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUNS == 0:\n",
    "  try:\n",
    "    %pprint\n",
    "    DEFAULT_GH_PROJECT=\"synavatelabs_project\"\n",
    "    GH_USER = 'snyata' #this is your git user name\n",
    "    NOTEBOOK_ERRORS = []\n",
    "    RUNS+=1\n",
    "    print(\"##### INITIAL SETUP COMPLETE ######\")\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# SET TO FALSE IF NOT ON GOOGLE COLAB\n",
    "COLAB = False\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if COLAB == False & torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Environment Colab == {COLAB}\\n\\n Your device is {device}\")\n",
    "if COLAB == False:\n",
    "    print(\"FOR GPU ON COLAB SET COLAB TO TRUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing\n",
    "### Logging and safe execution decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Callable, Any\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "#TODO - Add a directory for logs\n",
    "logging.basicConfig(level='logging.{}, LOG_LEVEL', filename=f'./data/logs/{PROJECT_NAME}_safe_apply_errors.log',\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "def safe_apply(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    A decorator to safely apply a function to a pandas DataFrame or Series column.\n",
    "\n",
    "    Args:\n",
    "        func: A function to be applied to a pandas DataFrame or Series column.\n",
    "\n",
    "    Returns:\n",
    "        The wrapped function with added exception handling.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        count_errors = 0\n",
    "        try:\n",
    "            # Assuming the first argument is always the DataFrame or Series\n",
    "            result = func(*args, **kwargs)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            count_errors += 1\n",
    "            if count_errors > 2:\n",
    "                logging.error(__name__)\n",
    "                sys.exit()\n",
    "            # Log the exception\n",
    "            logging.error(f\"Error applying function {func.__name__}: {e}\")\n",
    "            # Optionally, return None or a default value instead of raising\n",
    "            return None\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face Data\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "#load_dotenv()\n",
    "\n",
    "DATASET = \"Olec/cyber-threat-intelligence_v2\"\n",
    "ds = load_dataset(DATASET)\n",
    "###Turns the DATSET into a set of tensors for PyTorch\n",
    "#ds = ds.with_format(\"torch\", device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas Dataframe from the training dataset\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": ds['train'][\"id\"],\n",
    "    \"text\": ds[\"train\"][\"text\"],\n",
    "    \"entities\": ds[\"train\"][\"entities\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    if type(df) != pd.DataFrame:\n",
    "        logging.error(f\"####START OF PROCESSING: {df} is type {type(df)} ####\")\n",
    "    else:\n",
    "        print(f\"####START OF PROCESSING: {df} is type {type(df)} ####\\n\")\n",
    "    print(f\"START SHAPE:  {df.shape}\")\n",
    "    # Remove N/A and duplicate text, id\n",
    "    df['text'] = df['text'].drop_duplicates(inplace=True)\n",
    "    df['id'] = df['id'].drop_duplicates(inplace=True)\n",
    "    df_processed = df.dropna()\n",
    "    print(\"######  SHAPE OF YOUR DATA IS  ######\")\n",
    "    print(df_processed.shape)\n",
    "    if type(df) != pd.DataFrame:\n",
    "        logging.error(f\"####END OF PROCESSING: {df} is type {type(df)} ####\")\n",
    "    else:\n",
    "        print(f\"####END OF PROCESSING: {df} is type {type(df)} ####\")\n",
    "    print(\"######  END OF DATA CLEANING  ######\")    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm IProgress\n",
    "%pip install -q nltk\n",
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Text NLP\n",
    "\n",
    "import IProgress\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Variable For Operations on Entities\n",
    "# Helpful to work out exactly where in the data structure the item is\n",
    "\n",
    "\n",
    "#label_variable = df['entities'][0][2]['label']\n",
    "#label_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@safe_apply\n",
    "def reduce_text(text: pd.Series) -> pd.Series:\n",
    "  lemmatizer: nltk.WordNetLemmatizer = WordNetLemmatizer()\n",
    "  words: List = [lemmatizer.lemmatize(text) for words in text if words not in stopwords.words('english')]\n",
    "  words = ' '.join(words)\n",
    "  \n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE TEXT FUNCTION\n",
    "@safe_apply\n",
    "def tokenize_text(text):\n",
    "  for word in text:  \n",
    "    tokens = word_tokenize(word)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "  return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@safe_apply\n",
    "def extract_features(entities):\n",
    "    # Define the features you're interested in.\n",
    "    features_of_interest = [\n",
    "        \"attack-pattern\",\n",
    "        \"malware\",\n",
    "        \"ip\",\n",
    "        \"threat-actor\",\n",
    "        \"campaign\",\n",
    "        \"intrusion-set\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize an empty list to hold labels (features) found in entities.\n",
    "    features_found = []\n",
    "    \n",
    "    # Iterate through each entity dictionary in the entities list.\n",
    "    \n",
    "        # If the entity's label is one of the features of interest, append it to the list.\n",
    "    if entities[0][0][2]['label'] in features_of_interest:\n",
    "        features_found.append(entities[0][0][2]['label'])\n",
    "        \n",
    "    \n",
    "    # Return the list with labels found in entities.\n",
    "    return ', '.join(features_found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate attack patterns\n",
    "@safe_apply\n",
    "def isolate_attack_patterns(entities):\n",
    "    # Directly check 'label' in each entity dictionary\n",
    "    attack_patterns = [entities[0][2]['label'] for entity in entities if entity == 'attack-pattern']\n",
    "    attack_patterns = ', '.join(attack_patterns)\n",
    "    logging.INFO(\"SUCCESS ATTACK LABELS\")\n",
    "    return attack_patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@safe_apply\n",
    "def isolate_labels(data: pd.Series) -> pd.Series:\n",
    "    # Pulls the labels out of the entities dictionaries into it's own column\n",
    "    print(data)\n",
    "    data[\"raw_labels\"] = data['entities'][0][2]['labels']\n",
    "    logging.INFO(\"SUCCESS LABELS ISOLATION\")\n",
    "    return data[\"raw_labels\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ONE HOT ENCODING FUNCTION\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "@safe_apply\n",
    "def one_hot(data):\n",
    "    one_hot = OneHotEncoder()\n",
    "    labels_array = np.array(data).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot.fit_transform(labels_array)\n",
    "    logging.INFO(\"SUCCESS ONE HOT\")\n",
    "    return one_hot_encoded\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify\n",
    "@safe_apply\n",
    "def classify_text(text):\n",
    "    return nltk.classify(text)\n",
    "\n",
    "print (\"### CLASSIFY NOT IMPLEMENTED ###\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Procedure\n",
    "PROGRESS_APPLY FOR TRACKING OF PANDAS EXECUTIOn - SAFE_APPLY WILL AUTOMATICALLY LOG INCLUDING ERRORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARALLEL PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "ALL EXECUTION OF PROGRESS APPLYING TEXT PROCESSING ARE IN THIS STEP FOR THE ORION LIBRARY\n",
    "\n",
    "'''\n",
    "\n",
    "###RUN DATA CLEANING #####\n",
    "df = data_cleaning(df)\n",
    "\n",
    "def run_all_apply(df):\n",
    "\n",
    "    #Reduce text down using NLTK and apply from Pandas\n",
    "    df[\"text\"] = df[\"text\"].progress_apply(reduce_text)\n",
    "    print(\"Text reduced\")\n",
    "\n",
    "    # ATTACK PATTERNS\n",
    "    df['attack_patterns'] = df['entities'].progress_apply(isolate_attack_patterns)\n",
    "    print(\"Attack patterns done\")\n",
    "\n",
    "    # FEATURE EXTRACTION\n",
    "    # Apply the function to each row's 'entities' column and store the result in a new column.\n",
    "    df['features_found'] = df['entities'].progress_apply(extract_features)\n",
    "    print(\"features extracted\")\n",
    "\n",
    "    # THREAT LABELS\n",
    "\n",
    "    df['threat_labels'] = df['entities'].progress_apply(isolate_labels)\n",
    "    print(\"Threat Labels extracted\")\n",
    "\n",
    "    # ONE HOT ENCODING\n",
    "    #OneHot Encoding\n",
    "    df['one_hot_labels']= df['threat_labels'].progress_apply(one_hot)\n",
    "    print(\"One Hot Encoded\")\n",
    "\n",
    "    #Tokenize text\n",
    "    df[\"tokens\"] = df[\"text\"].progress_apply(tokenize_text)\n",
    "    print(\"Text tokenized\")\n",
    "    return df\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "PARALLEL= True\n",
    "if PARALLEL:\n",
    "    try:\n",
    "        # Create a multiprocessing pool\n",
    "        pool = multiprocessing.Pool()\n",
    "        result = pool.map(run_all_apply, df.itertuples(index=False))\n",
    "        \n",
    "        # Assuming 'result' is a list of dictionaries, you can directly create a DataFrame\n",
    "        df_result = pd.DataFrame(result)\n",
    "        \n",
    "        print(\"PARALLEL DONE - RESULT DF CREATED\")\n",
    "        \n",
    "        # Close the multiprocessing pool\n",
    "        pool.close()\n",
    "        # Wait for all worker processes to finish\n",
    "        pool.join()\n",
    "        df_result.head()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GCP Storage SOURCE BUCKET ###\n",
    "%env SOURCE_BUCKET=example-state-backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ..gcp.gcp_upload import upload_blob\n",
    "from datetime import datetime\n",
    "# Turn processed dataframe to csv \n",
    "csv_output = df.tocsv(f\"./processed/gretel-threat-intelligence-{datetime.now()}.csv\")\n",
    "%export DATA_FILE= csv_output\n",
    "# Upload blob\n",
    "upload_blob(\"gretel-threat-intelligence\", os.getenv(\"DATA_FILE\"), \"gretel-threat-intelligence.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage cp $DATA_FILE gs://$SOURCE_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgliner-mojo-IVCG5sOP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
